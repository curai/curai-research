trainer:
  device: cuda
  accumulate_grad_batches: 16
  max_epochs: 1
  gradient_clip_val: 0.0
  log_every_n_steps: 1
  evaluation_step: 200
  fp16: true
  task: mlm
checkpoint:
  dirpath: checkpoints
  verbose: true
  path:
dataset:
  _name_: wiki
  data: wikipedia
  subset: 20220301.en
  cache_dir: ../cache
  test_size: 0.0001 # 6458024 train samples & 646 dev samples
  tokenizer: bert-base-uncased # google/long-t5-tglobal-base for causal model & bert-base-uncased for bidirectional models
  batch_size: 2
  l_max: 4096
optimizer:
  _name_: adamw
  lr: 0.0001
  weight_decay: 0.0001
scheduler:
  _name_: cosine_warmup
  num_warmup_steps: 1000
  num_training_steps: 800000
embedding:
  rescale: true
  d_model: ${model.d_model}
  n_tokens: 30522
decoder: 
  tied: false
  d_output: 768
model:
  layer:
  - _name_: s4
    d_state: 64
    l_max: ${dataset.l_max}
    postact: glu
    dropout: ${...dropout}
    lr: ${optimizer.lr}
    n_ssm: 128
    bidirectional: true
  - _name_: s4
    d_state: 64
    l_max: ${dataset.l_max}
    postact: glu
    dropout: ${...dropout}
    lr: ${optimizer.lr}
    n_ssm: 128
    bidirectional: true
  - _name_: ff
    expand: 2
    activation: gelu
    dropout: ${...dropout}
  _name_: model
  prenorm: true
  transposed: false
  n_layers: 12
  d_model: 768
  residual: R
  pool:
    _name_: pool
    stride: 1
    expand: 1
  norm: layer
  dropout: 0.1
  tie_dropout: false
  track_norms: true
  dropinp: 0.0