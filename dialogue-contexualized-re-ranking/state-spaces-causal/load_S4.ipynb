{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.utils import registry\n",
    "import src.utils as utils\n",
    "from S4Models import S4LMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    \n",
    "    config=compose(config_name=\"config_wiki_lm.yaml\")\n",
    "    print(OmegaConf.to_yaml(config))\n",
    "    OmegaConf.set_struct(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = S4LMModel(config)\n",
    "model.to('cpu')\n",
    "state_dict = torch.load('artifacts/checkpoints:v144/pytorch_model.bin',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/long-t5-tglobal-base')\n",
    "text = \"\"\"Benedict Joseph Fenwick was born on September 3, 1782, at Beaverdam Manor in Leonardtown, Maryland,[1] to George Fenwick II, a planter and surveyor, and Margaret Fenwick n√©e Medley.[2] His paternal ancestors hailed from Northumberland in North East England. Benedict's great-great-great grandfather, Cuthbert Fenwick, emigrated to America in the 1633 expedition of the Ark and the Dove, and was one of the original Catholic settlers of the British Province of Maryland.[3] Benedict's older brother was Enoch Fenwick, who would also become a prominent Jesuit,[4] and his cousin was Edward Fenwick, who would become a Dominican and Bishop of Cincinnati.\"\"\"\n",
    "prompt = tokenizer(text,add_special_tokens=False,return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#model.to('cuda')\n",
    "for module in model.modules():\n",
    "    if hasattr(module, '_step_mode'): \n",
    "        module._step_mode = 'linear'\n",
    "for module in model.modules():\n",
    "    if hasattr(module, 'setup_step'): \n",
    "        module.setup_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "model._reset_state(torch.tensor(prompt[0]).unsqueeze(0).unsqueeze(1),device='cuda')\n",
    "output = []\n",
    "with torch.no_grad():\n",
    "    for word in tqdm(prompt.squeeze()):\n",
    "        pred = model.generate(word.unsqueeze(0).unsqueeze(1).to('cuda'))\n",
    "states = model._state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [tokenizer.convert_ids_to_tokens(torch.argmax(pred[0]).detach().cpu().unsqueeze(0).numpy())]\n",
    "model._state = states\n",
    "top_p = 0.90\n",
    "next_word = torch.argmax(pred[0]).detach().cpu().unsqueeze(0).numpy()\n",
    "with torch.no_grad():\n",
    "    for word in range(1800):\n",
    "        y_t = model.generate(torch.tensor(next_word).unsqueeze(0).to('cpu'))\n",
    "        probs = F.softmax(y_t, dim=-1)\n",
    "\n",
    "        # Optional: nucleus sampling\n",
    "        if top_p < 1.0:\n",
    "            sorted_probs = probs.sort(dim=-1, descending=True)\n",
    "            csum_probs = sorted_probs.values.cumsum(dim=-1) > top_p\n",
    "            csum_probs[..., 1:] = csum_probs[..., :-1].clone()\n",
    "            csum_probs[..., 0] = 0\n",
    "            indices_to_remove = torch.zeros_like(csum_probs)\n",
    "            indices_to_remove[torch.arange(sorted_probs.indices.shape[0])[:, None].repeat(1, sorted_probs.indices.shape[1]).flatten(), sorted_probs.indices.flatten()] = csum_probs.flatten()\n",
    "            y_t = y_t + indices_to_remove.int() * (-1e20)\n",
    "        next_word = Categorical(logits=y_t/0.9).sample()\n",
    "        output += tokenizer.convert_ids_to_tokens([next_word])\n",
    "        print(output[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
